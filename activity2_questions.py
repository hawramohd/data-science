# -*- coding: utf-8 -*-
"""Activity2_questions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fjLTav8NWMJuSP4XbaGvFqy8_v8sA723

# Activity #2 (5 marks)

Class Activity: Analyzing NYC Hyperlocal Air Quality Data with Spatial Join

# Objective:
- This activity reinforces concepts from Chapters 6 (Tables), 7 (Visualization), 8 (Functions and Tables), and 9 (Randomness). Students will analyze NYC hyperlocal air quality data using Python in a Jupyter Notebook (Google Colab). The focus will be on leveraging the Table abstraction wherever possible. Additionally, students will use spatial join techniques to combine air quality sensor readings with geographic data.

given a csv file containing longitude, latitude , and pm10  columns [Air Quality data](https://raw.githubusercontent.com/IsamAljawarneh/datasets/master/data/NYC_PM.csv) representing readings of low cost air quality sensor mounted on moving vehicles, in addition to a geojson file containing polygons representing administrative divisions of NYC city known as neighbourhoods [nyc_polygon.geojson](https://raw.githubusercontent.com/IsamAljawarneh/datasets/master/data/nyc_polygon.geojson).
# Dataset Description
- Air Quality Sensor Readings (NYC_PM.csv) :
Attributes: SensorID, time, temperature, humidity, pm25,
Focus attributes: temperature, humidity, pm1,pm25,pm10,
- City Polygons (nyc_polygon.geojson) :
Contains polygons representing neighborhoods or boroughs in NYC.
Used for spatially joining geographic information with air quality data.

## **part - A** preprocessing [0 marks]

do all tasks and the subtasks!

# Onboarding Code Provided
- The following code will be provided in an onboarding Jupyter Notebook to help students get started:
"""

'''from google.colab import drive
drive.mount('/content/drive')'''

"""import necessary libraries"""

import pandas as pd
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
from datascience import *
# %matplotlib inline
#path_data = '../../../assets/data/'
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
import numpy as np

"""###1. Read the CSV file containing PM sensor readings
 & Read the GeoJSON file containing neighborhood boundaries into a GeoDataFrame
"""

# Step 1: Read the CSV file containing PM10 sensor readings
pm10_data = pd.read_csv('https://raw.githubusercontent.com/IsamAljawarneh/datasets/master/data/NYC_PM.csv',index_col=False)

# Step 2: Read the GeoJSON file containing neighborhood boundaries into a GeoDataFrame
nyc_neighborhoods = gpd.read_file('https://raw.githubusercontent.com/IsamAljawarneh/datasets/master/data/nyc_polygon.geojson')

#pm10_data.dtypes

"""### 2. convert the csv into a geodataframe and join it (sjoin) with the geojson, assign a coordinate reference system (CRS) the csv geodataframe which is identical to that of the geojson file, then perform the join, the result is a geodataframe, convert it to dataframe, and select pm10, neighborhood columns in a new dataframe"""

pm10_gdf = gpd.GeoDataFrame(pm10_data, geometry=gpd.points_from_xy(pm10_data.longitude, pm10_data.latitude))
merged_data = gpd.sjoin(pm10_gdf, nyc_neighborhoods, how='inner', predicate='within')

merged_data.head()

#merged_data.dtypes

pollution_data = merged_data[['pm10','neighborhood']]

pollution_data.shape[0]

#merged_data.rename(columns={'neighborhood': 'neighborhood1'}, inplace=True)

type(pollution_data)

"""3. you need to convert</h1></section> from dataframe to Datascience Table. Use the following format: ```Table.from_df(df, keep_index=False)``` read more here
[create DS Table from DF](https://www.data8.org/datascience/_autosummary/datascience.tables.Table.from_df.html)

**N.B.** <font color='red'>NOW, perform all tasks using the table abstraction as we have learned in the class!</font>

the following is the opposite:

[Table.to_df](https://www.data8.org/datascience/_autosummary/datascience.tables.Table.to_df.html)

what is the maximum pm10 value
"""

pollution_data['pm10'].max()

"""what is the maximum pm10 value"""

pollution_data['pm10'].min()

joined_table = Table().from_df(pollution_data)

"""show the first few rows of the table?"""

joined_table.show(2)

"""print minimum and maximum pm10 values?"""

pm10 = joined_table.column('pm10')
min(pm10), max(pm10)

"""#Instructions for Students
-You task is to analyze NYC hyperlocal air quality data using the provided dataset. Complete the following tasks in your Jupyter Notebook. Each task is worth 1 mark , for a total of 5 marks . Use the Table abstraction wherever possible.

# Tasks

###Task 1: Explore the Joined Data Using Tables (1 Mark)
- Display the first 5 rows of the joined_table using .show().
- Print the number of rows and columns in the table.
- Identify any missing values in the table and handle them appropriately (e.g., drop rows with missing values or fill them with a default value).
"""

#Display the first 5 rows of the joined_table
joined_table.show(5)

#Number of columns and rows
print("Number of rows:", joined_table.num_rows)
print("Number of columns:", joined_table.num_columns)

#Handle missing values
missing_values = joined_table.where('pm10', are.equal_to(np.nan)).num_rows
print("Number of missing values:", missing_values)

"""# Task 2: Create Summary Statistics Using Table Operations (1 Mark)

- Compute summary statistics (mean, median, min, max) for the attributes temperature, humidity, and pm25 using table operations like .column() and .apply().
- Group the data by borough and calculate the average pm25 levels for each borough using .group() or .pivot().
- Display the results in a new table.
"""

#Compute summary statistics

#convert df to table
merged_table=Table().from_df(merged_data)

#create 3 tables of the stats for each
temperature_stats = merged_table.select('temperature').stats()
humidity_stats = merged_table.select('humidity').stats()
pm25_stats = merged_table.select('pm10').stats()

#merge the tables into 1 stats table
stats_table=temperature_stats.join('statistic',humidity_stats,'statistic').join('statistic',pm25_stats,'statistic')
stats_table.show()

"""# Task 3: Visualize the Data Using Table-Based Plots (1 Mark)

- Create a bar chart showing the average pm25 levels for each borough using .barh().
- Create a scatter plot to visualize the relationship between temperature and pm25 using .scatter(). Add appropriate labels and a title to the plot.
- Create a histogram of humidity values across all neighborhoods using .hist().
"""

#average pm25 levels bar chart
pm25_avg=merged_table.select(['pm25','borough']).group('borough',np.mean).sort('pm25 mean',descending=True).barh('borough','pm25 mean')

#scatter plot to visualize the relationship between temperature and pm25

#removing outliers
scatter_table=merged_table.where('pm25',are.below(600)).where('temperature',are.below(2000))
scatter_table.scatter('temperature', 'pm25')
plt.title('Temperature vs PM2.5')
plt.xlabel('Temperature')
plt.ylabel('PM2.5')
plt.show()

#histogram of humidity values across all neighborhoods

merged_table.hist('humidity',bins=20)
plt.title('Humidity Distribution')
plt.xlabel('Humidity')
plt.ylabel('Frequency')
plt.show()

"""# Task 4: Define and Use Functions with Tables (1 Mark)

- Write a function calculate_pm25_category(pm25) that categorizes PM2.5 levels as follows:
"Good" if pm25 < 12
"Moderate" if 12 <= pm25 < 35
"Unhealthy" if pm25 >= 35
- Apply this function to the pm25 column in the table using .apply() to create a new column called PM25_Category.
- Count the number of sensors in each PM2.5 category and display the results using .group().

"""

#air quality label function
def categorize_air_quality(pm25):
    if pm25 < 12:
        return 'Good'
    elif 12 <= pm25 < 35:
        return 'Moderate'
    else:
        return 'Unhealthy'

#applying the function to pm25 column
merged_table = merged_table.with_column('PM25_Category', merged_table.apply(categorize_air_quality, 'pm25'))

#sensors count in each category
sensor_count = merged_table.select(['SensorID','PM25_Category']).group('PM25_Category')
sensor_count.show()

"""# Task 5: Simulate Random Sampling Using Tables (1 Mark)
- Randomly sample 10% of the rows from the table without replacement using .sample().
- Calculate the mean pm25 level for the sampled data.
- Repeat the random sampling process 100 times and store the mean pm25 values from each iteration in a list.
- Create a histogram of the 100 mean pm25 values to visualize the distribution of sample means.
"""

#list to store the means
samples_mean=[]

#loop to generate samples and calculate means
for _ in range(100):
    sample = merged_table.sample(int(0.1*merged_table.num_rows),with_replacement=False)
    samples_mean.append(np.mean(sample.column('pm25')))


#converting list to array to create histogram
samples_mean_array = np.array(samples_mean)
plt.hist(samples_mean_array)  # Use plt.hist to create the histogram
plt.title('Distribution of Sample Means')
plt.xlabel('Mean PM2.5')
plt.ylabel('Frequency')
plt.show()

"""# Submission Guidelines
- Add a "Open in Colab" button at the top of your notebook using the following Markdown code:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo-path/notebook.ipynb)

- Upload your completed Jupyter Notebook to a GitHub repository.
- Submit the link to your GitHub repository in the Blackboard LMS along with the Jupyter solution file.
- <font color = red size = 6> ATTENTION!!! </font> Students are encouraged to work on groups, however the submission should be individual and each student should have her/his own unique final assignment solution, which is to be submitted in BB

# Grading Rubric
- Each task is worth 1 mark , based on the following criteria:

- Correctness : The solution produces the expected output using the Table abstraction .
- Clarity : Code is well-organized, readable, and includes comments explaining key steps.
- Creativity : Visualizations and analyses are presented in an engaging and insightful manner.

# Hints for Success
- Use the Table abstraction methods like .select(), .where(), .group(), .apply(), and .sample() for data manipulation.
- Refer to the slides and examples from book Chapters 6, 7, 8, and 9 for guidance on tables, functions, visualizations, and randomness.
- Test your code frequently to ensure it runs without errors.
"""